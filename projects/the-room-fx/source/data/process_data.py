#!/usr/bin/env python3
"""
THE Room FX - SNSãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒˆãƒ©ã‚¤ãƒ–åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ»é›†è¨ˆã‚’è¡Œã†
"""

import pandas as pd
import json
import re
from pathlib import Path
from datetime import datetime
from collections import Counter

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent
RAW_DIR = BASE_DIR / "raw"
PROCESSED_DIR = BASE_DIR / "processed"
SUMMARY_DIR = BASE_DIR / "summary"
ANALYSIS_DIR = BASE_DIR / "analysis"

# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«
INPUT_FILE = RAW_DIR / "export_AnyMindGroup_ãƒ“ã‚¸ãƒã‚¹ã‚¯ãƒ©ã‚¹æµ·å¤–_auOahTdC (1).csv"

# ãƒˆãƒ©ã‚¤ãƒ–åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
TRIBE_KEYWORDS = {
    "Business Traveler": [
        "business", "executive", "ceo", "entrepreneur", "corporate", "cfo", "cto",
        "founder", "director", "manager", "consultant", "professional", "lawyer",
        "attorney", "banker", "finance", "investment", "investor"
    ],
    "Travel Enthusiast": [
        "travel", "traveler", "traveller", "wanderlust", "explorer", "aviation",
        "avgeek", "miles", "points", "frequent flyer", "jetsetter", "nomad",
        "globe", "world", "adventure", "passport", "trip", "journey"
    ],
    "Luxury Lifestyle": [
        "luxury", "premium", "first class", "vip", "elite", "exclusive",
        "affluent", "lifestyle", "fashion", "designer", "style"
    ],
    "Tech/Digital": [
        "tech", "developer", "engineer", "software", "digital", "startup",
        "founder", "data", "ai", "product", "crypto", "web3", "coder",
        "programmer", "it "
    ],
    "Creative": [
        "creator", "writer", "author", "photographer", "artist", "designer",
        "filmmaker", "journalist", "blogger", "influencer", "content",
        "youtuber", "podcaster", "media"
    ],
    "Japan Interest": [
        "japan", "tokyo", "osaka", "kyoto", "anime", "manga", "æ—¥æœ¬", "æ±äº¬",
        "japanese", "nihon", "nippon", "sushi", "ramen", "samurai"
    ]
}

# ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
TOPIC_KEYWORDS = {
    "Seat & Comfort": [
        "seat", "comfort", "legroom", "flat bed", "suite", "pod", "privacy",
        "sleep", "rest", "lie-flat", "spacious", "room"
    ],
    "Food & Dining": [
        "food", "meal", "dining", "champagne", "wine", "menu", "chef",
        "breakfast", "lunch", "dinner", "catering", "cuisine"
    ],
    "Service": [
        "service", "crew", "attendant", "staff", "flight attendant", "hospitality",
        "friendly", "helpful", "rude", "excellent service"
    ],
    "Lounge": [
        "lounge", "priority", "access", "airport lounge", "spa", "shower",
        "buffet", "waiting"
    ],
    "Price & Miles": [
        "price", "miles", "points", "upgrade", "deal", "expensive", "worth",
        "value", "cost", "affordable", "redeem", "award"
    ],
    "Entertainment": [
        "entertainment", "movie", "wifi", "screen", "tv", "ife", "music"
    ]
}

# èˆªç©ºä¼šç¤¾ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
AIRLINE_KEYWORDS = {
    "American Airlines": ["american airlines", "americanair", "@aa"],
    "United Airlines": ["united airlines", "united ", "@united"],
    "Delta Air Lines": ["delta", "delta air"],
    "Lufthansa": ["lufthansa"],
    "Air France": ["air france", "airfrance"],
    "British Airways": ["british airways", "ba ", "@britishairways"],
    "Singapore Airlines": ["singapore airlines", "singaporeair"],
    "Emirates": ["emirates"],
    "Qatar Airways": ["qatar", "qatar airways"],
    "ANA": ["ana ", "all nippon", "å…¨æ—¥ç©º"],
    "JAL": ["jal ", "japan airlines", "æ—¥æœ¬èˆªç©º"],
    "Air Canada": ["air canada"],
    "Cathay Pacific": ["cathay pacific", "cathay"],
    "Korean Air": ["korean air"]
}

# åœ°åŸŸåˆ†é¡
REGION_MAPPING = {
    # TC1 - åŒ—ç±³
    "us": "TC1_NA", "ca": "TC1_NA", "mx": "TC1_NA",
    "united states": "TC1_NA", "canada": "TC1_NA", "mexico": "TC1_NA",
    # TC2 - æ¬§å·
    "uk": "TC2_EU", "gb": "TC2_EU", "united kingdom": "TC2_EU",
    "de": "TC2_EU", "germany": "TC2_EU",
    "fr": "TC2_EU", "france": "TC2_EU",
    "it": "TC2_EU", "italy": "TC2_EU",
    "es": "TC2_EU", "spain": "TC2_EU",
    "nl": "TC2_EU", "netherlands": "TC2_EU",
    "be": "TC2_EU", "belgium": "TC2_EU",
    "ch": "TC2_EU", "switzerland": "TC2_EU",
    "at": "TC2_EU", "austria": "TC2_EU",
    "se": "TC2_EU", "sweden": "TC2_EU",
    "no": "TC2_EU", "norway": "TC2_EU",
    "dk": "TC2_EU", "denmark": "TC2_EU",
    "fi": "TC2_EU", "finland": "TC2_EU",
    "ie": "TC2_EU", "ireland": "TC2_EU",
    "pt": "TC2_EU", "portugal": "TC2_EU",
    "pl": "TC2_EU", "poland": "TC2_EU",
    # TC3 - APAC
    "jp": "TC3_APAC", "japan": "TC3_APAC",
    "cn": "TC3_APAC", "china": "TC3_APAC",
    "kr": "TC3_APAC", "south korea": "TC3_APAC",
    "sg": "TC3_APAC", "singapore": "TC3_APAC",
    "th": "TC3_APAC", "thailand": "TC3_APAC",
    "au": "TC3_APAC", "australia": "TC3_APAC",
    "hk": "TC3_APAC", "hong kong": "TC3_APAC",
    "tw": "TC3_APAC", "taiwan": "TC3_APAC",
    "id": "TC3_APAC", "indonesia": "TC3_APAC",
    "my": "TC3_APAC", "malaysia": "TC3_APAC",
    "vn": "TC3_APAC", "vietnam": "TC3_APAC",
    "in": "TC3_APAC", "india": "TC3_APAC",
}

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›½
TARGET_COUNTRIES = {
    "us": "US", "united states": "US",
    "uk": "UK", "gb": "UK", "united kingdom": "UK",
    "fr": "FR", "france": "FR",
    "de": "DE", "germany": "DE",
    "it": "IT", "italy": "IT",
    "ca": "CA", "canada": "CA"
}


def load_data():
    """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"Loading data from {INPUT_FILE}...")
    df = pd.read_csv(INPUT_FILE, encoding='utf-8', low_memory=False)
    print(f"Loaded {len(df)} rows")
    return df


def classify_region(country_code, country_name):
    """åœ°åŸŸã‚’åˆ†é¡ã™ã‚‹"""
    if pd.isna(country_code) and pd.isna(country_name):
        return "Unknown"

    # å›½ã‚³ãƒ¼ãƒ‰ã§åˆ¤å®š
    if pd.notna(country_code):
        cc = str(country_code).lower().strip()
        if cc in REGION_MAPPING:
            return REGION_MAPPING[cc]

    # å›½åã§åˆ¤å®š
    if pd.notna(country_name):
        cn = str(country_name).lower().strip()
        if cn in REGION_MAPPING:
            return REGION_MAPPING[cn]

    return "Other"


def classify_target_country(country_code, country_name):
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›½ã‚’åˆ†é¡ã™ã‚‹"""
    if pd.isna(country_code) and pd.isna(country_name):
        return "Other"

    if pd.notna(country_code):
        cc = str(country_code).lower().strip()
        if cc in TARGET_COUNTRIES:
            return TARGET_COUNTRIES[cc]

    if pd.notna(country_name):
        cn = str(country_name).lower().strip()
        if cn in TARGET_COUNTRIES:
            return TARGET_COUNTRIES[cn]

    return "Other"


def classify_tribe(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ©ã‚¤ãƒ–ã‚’åˆ†é¡ã™ã‚‹"""
    if pd.isna(text):
        return []

    text_lower = str(text).lower()
    tribes = []

    for tribe, keywords in TRIBE_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text_lower:
                tribes.append(tribe)
                break

    return tribes


def extract_topics(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã™ã‚‹"""
    if pd.isna(text):
        return []

    text_lower = str(text).lower()
    topics = []

    for topic, keywords in TOPIC_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text_lower:
                topics.append(topic)
                break

    return topics


def extract_airline(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰èˆªç©ºä¼šç¤¾ã‚’æŠ½å‡ºã™ã‚‹"""
    if pd.isna(text):
        return "Other"

    text_lower = str(text).lower()

    for airline, keywords in AIRLINE_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text_lower:
                return airline

    return "Other"


def parse_date(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    if pd.isna(date_str):
        return None

    try:
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YY/MM/DD HH:MM:SS
        return datetime.strptime(str(date_str), "%y/%m/%d %H:%M:%S")
    except ValueError:
        return None


def process_data(df):
    """ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ãƒ»åŠ å·¥ã™ã‚‹"""
    print("Processing data...")

    # ã‚«ãƒ©ãƒ åã®ç¢ºèª
    print(f"Columns: {df.columns.tolist()[:20]}...")

    # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    clean_df = pd.DataFrame()

    # åŸºæœ¬æƒ…å ±
    clean_df['id'] = df['url']
    clean_df['published_at'] = df['published'].apply(parse_date)
    clean_df['content'] = df['content']
    clean_df['lang'] = df['lang']
    clean_df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').fillna(0)

    # è‘—è€…æƒ…å ±
    clean_df['author_name'] = df['extra_author_attributes.name']
    clean_df['author_gender'] = df['extra_author_attributes.gender']
    clean_df['author_followers'] = pd.to_numeric(
        df['source_extended_attributes.twitter_followers'], errors='coerce'
    ).fillna(0).astype(int)
    clean_df['author_description'] = df['extra_author_attributes.description']
    clean_df['author_url'] = df['extra_author_attributes.url']

    # åœ°ç†æƒ…å ±
    clean_df['country'] = df['extra_article_attributes.world_data.country']
    clean_df['country_code'] = df['extra_article_attributes.world_data.country_code']
    clean_df['city'] = df['extra_article_attributes.world_data.city']

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™
    clean_df['impressions'] = pd.to_numeric(
        df['article_extended_attributes.twitter_impressions'], errors='coerce'
    ).fillna(0).astype(int)
    clean_df['likes'] = pd.to_numeric(
        df['article_extended_attributes.twitter_likes'], errors='coerce'
    ).fillna(0).astype(int)
    clean_df['retweets'] = pd.to_numeric(
        df['article_extended_attributes.twitter_retweets'], errors='coerce'
    ).fillna(0).astype(int)
    clean_df['replies'] = pd.to_numeric(
        df['article_extended_attributes.twitter_replies'], errors='coerce'
    ).fillna(0).astype(int)
    clean_df['engagement'] = pd.to_numeric(df['engagement'], errors='coerce').fillna(0).astype(int)
    clean_df['reach'] = pd.to_numeric(df['reach'], errors='coerce').fillna(0).astype(int)

    # åœ°åŸŸåˆ†é¡
    clean_df['region'] = clean_df.apply(
        lambda row: classify_region(row['country_code'], row['country']), axis=1
    )
    clean_df['target_country'] = clean_df.apply(
        lambda row: classify_target_country(row['country_code'], row['country']), axis=1
    )

    # ãƒˆãƒ©ã‚¤ãƒ–åˆ†é¡ï¼ˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰ï¼‰
    clean_df['tribes'] = clean_df['author_description'].apply(classify_tribe)
    clean_df['tribe_primary'] = clean_df['tribes'].apply(
        lambda x: x[0] if x else "Unclassified"
    )

    # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆæŠ•ç¨¿å†…å®¹ã‹ã‚‰ï¼‰
    clean_df['topics'] = clean_df['content'].apply(extract_topics)

    # èˆªç©ºä¼šç¤¾æŠ½å‡º
    clean_df['airline'] = clean_df['content'].apply(extract_airline)

    print(f"Processed {len(clean_df)} rows")
    return clean_df


def save_clean_data(clean_df):
    """ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹"""
    print("Saving clean data...")

    # CSVä¿å­˜ï¼ˆtribesã¨topicsã¯JSONæ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
    csv_df = clean_df.copy()
    csv_df['tribes'] = csv_df['tribes'].apply(json.dumps)
    csv_df['topics'] = csv_df['topics'].apply(json.dumps)
    csv_df.to_csv(PROCESSED_DIR / "clean.csv", index=False, encoding='utf-8')

    # JSONä¿å­˜
    json_data = clean_df.to_dict(orient='records')
    for record in json_data:
        if pd.notna(record.get('published_at')):
            record['published_at'] = record['published_at'].isoformat()
    with open(PROCESSED_DIR / "clean.json", 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåœ°åŸŸã®ã¿æŠ½å‡º
    target_df = clean_df[clean_df['target_country'] != 'Other'].copy()
    target_csv = target_df.copy()
    target_csv['tribes'] = target_csv['tribes'].apply(json.dumps)
    target_csv['topics'] = target_csv['topics'].apply(json.dumps)
    target_csv.to_csv(PROCESSED_DIR / "target_region.csv", index=False, encoding='utf-8')

    target_json = target_df.to_dict(orient='records')
    for record in target_json:
        if pd.notna(record.get('published_at')):
            record['published_at'] = record['published_at'].isoformat()
    with open(PROCESSED_DIR / "target_region.json", 'w', encoding='utf-8') as f:
        json.dump(target_json, f, ensure_ascii=False, indent=2)

    print(f"Saved clean.csv ({len(clean_df)} rows)")
    print(f"Saved target_region.csv ({len(target_df)} rows)")


def generate_author_profiles(clean_df):
    """è‘—è€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating author profiles...")

    # è‘—è€…ã”ã¨ã«é›†è¨ˆ
    author_stats = clean_df.groupby('author_name').agg({
        'author_description': 'first',
        'author_followers': 'first',
        'author_url': 'first',
        'country': 'first',
        'target_country': 'first',
        'region': 'first',
        'id': 'count',
        'sentiment': 'mean',
        'engagement': 'sum',
        'impressions': 'sum',
        'tribe_primary': 'first'
    }).reset_index()

    author_stats.columns = [
        'author_name', 'author_description', 'followers', 'author_url',
        'country', 'target_country', 'region', 'post_count',
        'avg_sentiment', 'total_engagement', 'total_impressions', 'tribe'
    ]

    # ã‚½ãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°é †ï¼‰
    author_stats = author_stats.sort_values('followers', ascending=False)

    # ä¿å­˜
    author_stats.to_csv(SUMMARY_DIR / "author_profiles.csv", index=False, encoding='utf-8')

    print(f"Generated author_profiles.csv ({len(author_stats)} authors)")
    return author_stats


def generate_tribe_distribution(clean_df):
    """ãƒˆãƒ©ã‚¤ãƒ–åˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating tribe distribution...")

    # ãƒˆãƒ©ã‚¤ãƒ–ã”ã¨ã«é›†è¨ˆ
    tribe_stats = clean_df.groupby('tribe_primary').agg({
        'id': 'count',
        'author_name': 'nunique',
        'author_followers': 'mean',
        'impressions': 'sum',
        'engagement': 'sum',
        'sentiment': 'mean'
    }).reset_index()

    tribe_stats.columns = [
        'tribe', 'post_count', 'unique_authors', 'avg_followers',
        'total_impressions', 'total_engagement', 'avg_sentiment'
    ]

    # ã‚½ãƒ¼ãƒˆ
    tribe_stats = tribe_stats.sort_values('post_count', ascending=False)

    # ä¿å­˜
    tribe_stats.to_csv(SUMMARY_DIR / "tribe_distribution.csv", index=False, encoding='utf-8')

    print(f"Generated tribe_distribution.csv")
    return tribe_stats


def generate_region_tribe_matrix(clean_df):
    """åœ°åŸŸÃ—ãƒˆãƒ©ã‚¤ãƒ–ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating region-tribe matrix...")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåœ°åŸŸã®ã¿
    target_df = clean_df[clean_df['target_country'] != 'Other']

    # ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆæŠ•ç¨¿æ•°ï¼‰
    matrix = pd.crosstab(
        target_df['target_country'],
        target_df['tribe_primary'],
        values=target_df['id'],
        aggfunc='count'
    ).fillna(0).astype(int)

    # ä¿å­˜
    matrix.to_csv(SUMMARY_DIR / "region_tribe_matrix.csv", encoding='utf-8')

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®é›†è¨ˆã‚‚
    matrix_engagement = pd.crosstab(
        target_df['target_country'],
        target_df['tribe_primary'],
        values=target_df['engagement'],
        aggfunc='sum'
    ).fillna(0).astype(int)

    matrix_engagement.to_csv(SUMMARY_DIR / "region_tribe_matrix_engagement.csv", encoding='utf-8')

    print(f"Generated region_tribe_matrix.csv")
    return matrix


def generate_topic_analysis(clean_df):
    """ãƒˆãƒ”ãƒƒã‚¯åˆ†æã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating topic analysis...")

    # ãƒˆãƒ”ãƒƒã‚¯ã‚’å±•é–‹
    topic_rows = []
    for _, row in clean_df.iterrows():
        for topic in row['topics']:
            topic_rows.append({
                'topic': topic,
                'impressions': row['impressions'],
                'engagement': row['engagement'],
                'sentiment': row['sentiment'],
                'region': row['region'],
                'target_country': row['target_country']
            })

    if not topic_rows:
        print("No topics found")
        return None

    topic_df = pd.DataFrame(topic_rows)

    # ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã«é›†è¨ˆ
    topic_stats = topic_df.groupby('topic').agg({
        'impressions': ['count', 'sum', 'mean'],
        'engagement': 'sum',
        'sentiment': 'mean'
    }).reset_index()

    topic_stats.columns = [
        'topic', 'mention_count', 'total_impressions', 'avg_impressions',
        'total_engagement', 'avg_sentiment'
    ]

    topic_stats = topic_stats.sort_values('mention_count', ascending=False)

    # ä¿å­˜
    topic_stats.to_csv(SUMMARY_DIR / "topic_analysis.csv", index=False, encoding='utf-8')

    print(f"Generated topic_analysis.csv")
    return topic_stats


def generate_daily_stats(clean_df):
    """æ—¥åˆ¥çµ±è¨ˆã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating daily stats...")

    # æ—¥ä»˜ãŒãªã„è¡Œã‚’é™¤å¤–
    daily_df = clean_df[clean_df['published_at'].notna()].copy()
    daily_df['date'] = daily_df['published_at'].dt.date

    # æ—¥åˆ¥é›†è¨ˆ
    daily_stats = daily_df.groupby('date').agg({
        'id': 'count',
        'impressions': 'sum',
        'engagement': 'sum',
        'sentiment': 'mean'
    }).reset_index()

    daily_stats.columns = ['date', 'post_count', 'total_impressions', 'total_engagement', 'avg_sentiment']
    daily_stats = daily_stats.sort_values('date')

    # ä¿å­˜
    daily_stats.to_csv(SUMMARY_DIR / "daily_stats.csv", index=False, encoding='utf-8')

    print(f"Generated daily_stats.csv")
    return daily_stats


def generate_influencer_list(clean_df, author_stats, min_followers=10000):
    """ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    print(f"Generating influencer list (min followers: {min_followers})...")

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿
    influencers = author_stats[author_stats['followers'] >= min_followers].copy()

    # ãƒˆãƒƒãƒ—æŠ•ç¨¿ã‚’å–å¾—
    def get_top_post(author_name):
        author_posts = clean_df[clean_df['author_name'] == author_name]
        if len(author_posts) == 0:
            return None
        top_post = author_posts.loc[author_posts['engagement'].idxmax()]
        return top_post['id']

    influencers['top_post'] = influencers['author_name'].apply(get_top_post)

    # ã‚½ãƒ¼ãƒˆ
    influencers = influencers.sort_values('followers', ascending=False)

    # ä¿å­˜
    influencers.to_csv(ANALYSIS_DIR / "influencers_by_tribe.csv", index=False, encoding='utf-8')

    # JSONç‰ˆ
    influencers.to_json(
        ANALYSIS_DIR / "influencers_by_tribe.json",
        orient='records',
        force_ascii=False,
        indent=2
    )

    print(f"Generated influencers_by_tribe.csv ({len(influencers)} influencers)")
    return influencers


def generate_top_voices(clean_df):
    """ãƒˆãƒƒãƒ—ç™ºè¨€è€…ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating top voices...")

    # ãƒˆãƒ©ã‚¤ãƒ–ã”ã¨ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä¸Šä½ã‚’å–å¾—
    top_voices_list = []

    for tribe in clean_df['tribe_primary'].unique():
        tribe_df = clean_df[clean_df['tribe_primary'] == tribe]
        tribe_authors = tribe_df.groupby('author_name').agg({
            'engagement': 'sum',
            'impressions': 'sum',
            'author_followers': 'first',
            'country': 'first',
            'id': 'count'
        }).reset_index()

        tribe_authors.columns = [
            'author_name', 'total_engagement', 'total_impressions',
            'followers', 'country', 'post_count'
        ]

        # ä¸Šä½10å
        top_10 = tribe_authors.nlargest(10, 'total_engagement')
        top_10['tribe'] = tribe
        top_voices_list.append(top_10)

    if top_voices_list:
        top_voices = pd.concat(top_voices_list, ignore_index=True)
        top_voices.to_csv(ANALYSIS_DIR / "top_voices.csv", index=False, encoding='utf-8')
        print(f"Generated top_voices.csv")
        return top_voices

    return None


def generate_airline_stats(clean_df):
    """èˆªç©ºä¼šç¤¾åˆ¥çµ±è¨ˆã‚’ç”Ÿæˆã™ã‚‹"""
    print("Generating airline stats...")

    airline_stats = clean_df.groupby('airline').agg({
        'id': 'count',
        'impressions': 'sum',
        'engagement': 'sum',
        'sentiment': 'mean',
        'author_name': 'nunique'
    }).reset_index()

    airline_stats.columns = [
        'airline', 'post_count', 'total_impressions', 'total_engagement',
        'avg_sentiment', 'unique_authors'
    ]

    # ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡
    for airline in airline_stats['airline'].unique():
        airline_df = clean_df[clean_df['airline'] == airline]
        total = len(airline_df)
        if total > 0:
            positive_rate = len(airline_df[airline_df['sentiment'] > 0]) / total
            negative_rate = len(airline_df[airline_df['sentiment'] < 0]) / total
            airline_stats.loc[airline_stats['airline'] == airline, 'positive_rate'] = positive_rate
            airline_stats.loc[airline_stats['airline'] == airline, 'negative_rate'] = negative_rate

    airline_stats = airline_stats.sort_values('post_count', ascending=False)

    # ä¿å­˜
    airline_stats.to_csv(SUMMARY_DIR / "airline_stats.csv", index=False, encoding='utf-8')

    print(f"Generated airline_stats.csv")
    return airline_stats


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("THE Room FX - SNS Data Processing")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data()

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    clean_df = process_data(df)

    # ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    save_clean_data(clean_df)

    # è‘—è€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    author_stats = generate_author_profiles(clean_df)

    # ãƒˆãƒ©ã‚¤ãƒ–åˆ†å¸ƒç”Ÿæˆ
    generate_tribe_distribution(clean_df)

    # åœ°åŸŸÃ—ãƒˆãƒ©ã‚¤ãƒ–ãƒãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
    generate_region_tribe_matrix(clean_df)

    # ãƒˆãƒ”ãƒƒã‚¯åˆ†æç”Ÿæˆ
    generate_topic_analysis(clean_df)

    # æ—¥åˆ¥çµ±è¨ˆç”Ÿæˆ
    generate_daily_stats(clean_df)

    # ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒªã‚¹ãƒˆç”Ÿæˆ
    generate_influencer_list(clean_df, author_stats)

    # ãƒˆãƒƒãƒ—ç™ºè¨€è€…ç”Ÿæˆ
    generate_top_voices(clean_df)

    # èˆªç©ºä¼šç¤¾åˆ¥çµ±è¨ˆç”Ÿæˆ
    generate_airline_stats(clean_df)

    print("=" * 60)
    print("Processing complete!")
    print("=" * 60)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“Š Data Summary:")
    print(f"  Total posts: {len(clean_df)}")
    print(f"  Unique authors: {clean_df['author_name'].nunique()}")
    print(f"  Date range: {clean_df['published_at'].min()} - {clean_df['published_at'].max()}")

    print("\nğŸŒ Region Distribution:")
    print(clean_df['region'].value_counts().to_string())

    print("\nğŸ¯ Target Country Distribution:")
    print(clean_df['target_country'].value_counts().to_string())

    print("\nğŸ‘¥ Tribe Distribution:")
    print(clean_df['tribe_primary'].value_counts().to_string())

    print("\nâœˆï¸ Top Airlines Mentioned:")
    print(clean_df['airline'].value_counts().head(10).to_string())


if __name__ == "__main__":
    main()
